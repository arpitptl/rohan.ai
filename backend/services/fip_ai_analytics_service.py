import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from utils.logger import logger

# Import our custom services
from services.historical_analyzer import PrometheusHistoricalAnalyzer
from services.enhanced_bedrock_service import EnhancedBedrockService, PredictionResult, Alert

@dataclass
class AnalyticsResult:
    """Complete analytics result combining all AI insights"""
    timestamp: str
    time_range_analyzed: Dict
    historical_patterns: Dict
    predictions: Dict[str, PredictionResult]
    proactive_alerts: List[Alert]
    business_insights: Dict
    maintenance_windows: Dict
    summary: Dict

class FIPAIAnalyticsService:
    """
    Main service that orchestrates FIP AI analytics using historical data
    and AWS Bedrock for intelligent insights and predictions
    """
    
    def __init__(self, 
                 prometheus_url: str = "http://victoriametrics:8428",
                 use_real_bedrock: bool = False,
                 bedrock_region: str = 'us-east-1'):
        
        self.logger = logger
        
        # Initialize component services
        self.historical_analyzer = PrometheusHistoricalAnalyzer(prometheus_url)
        self.bedrock_service = EnhancedBedrockService(
            use_mock=not use_real_bedrock, 
            region_name=bedrock_region
        )
        
        # Cache for storing results
        self._cache = {}
        self._cache_ttl = 300  # 5 minutes
        
        self.logger.info("ðŸš€ FIP AI Analytics Service initialized")
    
    async def generate_comprehensive_analysis(self, 
                                            days_back: int = 7,
                                            prediction_horizon: str = "24h",
                                            include_current_metrics: bool = True) -> AnalyticsResult:
        """
        Generate comprehensive AI-powered analysis of FIP performance
        
        Args:
            days_back: Number of days of historical data to analyze
            prediction_horizon: Time window for predictions (1h, 6h, 24h, 7d)
            include_current_metrics: Whether to include current real-time metrics
            
        Returns:
            Complete analytics result with patterns, predictions, and insights
        """
        
        self.logger.info(f"ðŸ” Starting comprehensive FIP analysis for {days_back} days")
        
        try:
            # Step 1: Extract historical data
            self.logger.info("ðŸ“Š Extracting historical metrics...")
            historical_data = self.historical_analyzer.extract_historical_data(
                days_back=days_back,
                step="15m"  # 15-minute resolution for detailed analysis
            )
            
            if not any(not df.empty for df in historical_data.values()):
                raise Exception("No historical data available for analysis")
            
            # Step 2: Calculate features and patterns
            self.logger.info("ðŸ§® Calculating ML features...")
            fip_features = self.historical_analyzer.calculate_features(historical_data)
            
            # Step 3: Detect maintenance windows
            self.logger.info("ðŸ”§ Detecting maintenance patterns...")
            maintenance_windows = self.historical_analyzer.detect_maintenance_windows(historical_data)
            
            # Step 4: Generate comprehensive report
            self.logger.info("ðŸ“‹ Generating comprehensive report...")
            comprehensive_report = self.historical_analyzer.generate_summary_report(
                historical_data, fip_features, maintenance_windows
            )
            
            # Step 5: AI Pattern Analysis using Bedrock
            self.logger.info("ðŸ¤– Analyzing patterns with AI...")
            historical_patterns = self.bedrock_service.analyze_historical_patterns(comprehensive_report)
            
            # Step 6: Generate AI predictions
            self.logger.info("ðŸ”® Generating AI predictions...")
            predictions = self.bedrock_service.predict_downtime_events(
                comprehensive_report, prediction_horizon
            )
            
            # Step 7: Get current metrics if requested
            current_metrics = {}
            if include_current_metrics:
                self.logger.info("â±ï¸ Fetching current metrics...")
                current_metrics = await self._get_current_metrics()
            
            # Step 8: Generate proactive alerts
            self.logger.info("ðŸš¨ Generating proactive alerts...")
            proactive_alerts = self.bedrock_service.generate_proactive_alerts(
                comprehensive_report, current_metrics
            )
            
            # Step 9: Generate business insights
            self.logger.info("ðŸ’¼ Generating business insights...")
            business_insights = self.bedrock_service.generate_business_insights(
                comprehensive_report, predictions
            )
            
            # Step 10: Create final result
            result = AnalyticsResult(
                timestamp=datetime.utcnow().isoformat(),
                time_range_analyzed=comprehensive_report.get('time_range_analyzed', {}),
                historical_patterns=historical_patterns,
                predictions=predictions,
                proactive_alerts=proactive_alerts,
                business_insights=business_insights,
                maintenance_windows=maintenance_windows,
                summary=self._generate_executive_summary(
                    comprehensive_report, predictions, proactive_alerts, business_insights
                )
            )
            
            self.logger.info("âœ… Comprehensive analysis completed successfully")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Error in comprehensive analysis: {e}")
            raise
    
    async def generate_quick_insights(self, 
                                    days_back: int = 3,
                                    fip_filter: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Generate quick insights for immediate operational decisions
        
        Args:
            days_back: Number of days to analyze (shorter for quick analysis)
            fip_filter: List of specific FIPs to analyze (None for all)
            
        Returns:
            Quick insights focused on immediate actions needed
        """
        
        self.logger.info(f"âš¡ Generating quick insights for {days_back} days")
        
        try:
            # Extract recent data only
            historical_data = self.historical_analyzer.extract_historical_data(
                days_back=days_back,
                step="1h"  # Hourly resolution for quick analysis
            )
            
            # Filter data if specific FIPs requested
            if fip_filter:
                filtered_data = {}
                for metric_name, df in historical_data.items():
                    if not df.empty:
                        filtered_df = df[df['fip_name'].isin(fip_filter)]
                        filtered_data[metric_name] = filtered_df
                historical_data = filtered_data
            
            # Quick feature calculation
            fip_features = self.historical_analyzer.calculate_features(historical_data)
            
            # Generate quick report
            maintenance_windows = self.historical_analyzer.detect_maintenance_windows(historical_data)
            quick_report = self.historical_analyzer.generate_summary_report(
                historical_data, fip_features, maintenance_windows
            )
            
            # Quick AI analysis
            quick_patterns = self.bedrock_service.analyze_historical_patterns(quick_report)
            quick_predictions = self.bedrock_service.predict_downtime_events(quick_report, "6h")
            
            # Focus on immediate actions
            immediate_alerts = [
                alert for alert in self.bedrock_service.generate_proactive_alerts(quick_report, {})
                if alert.severity in ['critical', 'warning']
            ]
            
            return {
                'timestamp': datetime.utcnow().isoformat(),
                'analysis_scope': f"{days_back} days, {len(fip_features)} FIPs",
                'immediate_concerns': self._extract_immediate_concerns(quick_patterns, quick_predictions),
                'critical_alerts': [asdict(alert) for alert in immediate_alerts[:5]],
                'quick_recommendations': self._generate_quick_recommendations(quick_predictions),
                'risk_summary': self._calculate_risk_summary(quick_predictions),
                'next_analysis_recommended': (datetime.utcnow() + timedelta(hours=6)).isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Error in quick insights: {e}")
            raise
    
    async def monitor_predictions_accuracy(self, 
                                         prediction_window_hours: int = 24) -> Dict[str, Any]:
        """
        Monitor the accuracy of previous predictions to improve the model
        
        Args:
            prediction_window_hours: How far back to check prediction accuracy
            
        Returns:
            Accuracy metrics and model performance insights
        """
        
        self.logger.info(f"ðŸ“ˆ Monitoring prediction accuracy for {prediction_window_hours} hours")
        
        try:
            # Get historical predictions from cache/database (would need to implement storage)
            # For now, we'll generate a mock accuracy report
            
            accuracy_metrics = {
                'timestamp': datetime.utcnow().isoformat(),
                'analysis_window_hours': prediction_window_hours,
                'prediction_accuracy': {
                    'overall_accuracy': 0.87,  # 87% accuracy
                    'high_confidence_accuracy': 0.94,  # 94% for high confidence predictions
                    'medium_confidence_accuracy': 0.83,  # 83% for medium confidence
                    'low_confidence_accuracy': 0.71   # 71% for low confidence
                },
                'false_positive_rate': 0.08,  # 8% false positives
                'false_negative_rate': 0.05,  # 5% false negatives
                'early_warning_effectiveness': 0.91,  # 91% of issues detected early
                'business_impact_accuracy': {
                    'user_impact_estimation': 0.89,
                    'cost_estimation': 0.76,
                    'timeline_accuracy': 0.82
                },
                'model_improvements_suggested': [
                    "Increase weight of recent anomaly patterns",
                    "Improve correlation analysis between FIPs",
                    "Refine business impact calculation models",
                    "Add seasonal adjustment factors"
                ],
                'next_model_update_recommended': (datetime.utcnow() + timedelta(days=7)).isoformat()
            }
            
            return accuracy_metrics
            
        except Exception as e:
            self.logger.error(f"âŒ Error monitoring prediction accuracy: {e}")
            raise
    
    async def generate_scheduled_report(self, 
                                      report_type: str = "daily",
                                      recipients: List[str] = None) -> Dict[str, Any]:
        """
        Generate scheduled reports for different stakeholders
        
        Args:
            report_type: Type of report (daily, weekly, monthly)
            recipients: List of recipient types (executives, operations, technical)
            
        Returns:
            Formatted report for the specified audience
        """
        
        days_map = {"daily": 1, "weekly": 7, "monthly": 30}
        days_back = days_map.get(report_type, 7)
        
        self.logger.info(f"ðŸ“Š Generating {report_type} scheduled report")
        
        try:
            # Generate full analysis
            full_analysis = await self.generate_comprehensive_analysis(
                days_back=days_back,
                prediction_horizon="24h"
            )
            
            # Format for different audiences
            report = {
                'report_metadata': {
                    'type': report_type,
                    'generated_at': datetime.utcnow().isoformat(),
                    'period_analyzed': f"Last {days_back} days",
                    'recipients': recipients or ['operations']
                },
                'executive_summary': full_analysis.summary,
                'key_metrics': self._extract_key_metrics(full_analysis),
                'critical_issues': self._extract_critical_issues(full_analysis),
                'predictions_summary': self._summarize_predictions(full_analysis.predictions),
                'business_insights': full_analysis.business_insights.get('executive_summary', {}),
                'recommended_actions': self._prioritize_actions(full_analysis),
                'appendix': {
                    'detailed_patterns': full_analysis.historical_patterns,
                    'maintenance_schedule': full_analysis.maintenance_windows,
                    'technical_details': {
                        'data_quality': self._assess_data_quality(full_analysis),
                        'model_confidence': self._calculate_model_confidence(full_analysis),
                        'alert_breakdown': self._categorize_alerts(full_analysis.proactive_alerts)
                    }
                }
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"âŒ Error generating scheduled report: {e}")
            raise
    
    async def _get_current_metrics(self) -> Dict[str, Any]:
        """Get current real-time metrics from the metrics service"""
        try:
            # This would integrate with your existing MetricsService
            from services.metrics_service import MetricsService
            metrics_service = MetricsService()
            current_data = metrics_service.get_all_fips_status()
            
            return current_data
        except Exception as e:
            self.logger.warning(f"Could not fetch current metrics: {e}")
            return {}
    
    def _generate_executive_summary(self, 
                                  comprehensive_report: Dict,
                                  predictions: Dict[str, PredictionResult],
                                  alerts: List[Alert],
                                  business_insights: Dict) -> Dict[str, Any]:
        """Generate executive summary of the analysis"""
        
        # Count risk levels
        high_risk_fips = sum(1 for pred in predictions.values() if pred.downtime_probability > 0.7)
        medium_risk_fips = sum(1 for pred in predictions.values() if 0.4 <= pred.downtime_probability <= 0.7)
        
        # Count alert severities
        critical_alerts = sum(1 for alert in alerts if alert.severity == 'critical')
        warning_alerts = sum(1 for alert in alerts if alert.severity == 'warning')
        
        # Calculate total business impact
        total_users_at_risk = sum(pred.business_impact.get('affected_users', 0) for pred in predictions.values())
        total_revenue_risk = sum(pred.business_impact.get('revenue_impact_inr', 0) for pred in predictions.values())
        
        # System health assessment
        system_summary = comprehensive_report.get('system_summary', {})
        avg_health = system_summary.get('performance_overview', {}).get('avg_system_health', 0)
        
        if avg_health > 85 and critical_alerts == 0:
            overall_status = "healthy"
            status_description = "System operating within normal parameters"
        elif avg_health > 70 and critical_alerts <= 2:
            overall_status = "stable_with_concerns"
            status_description = "System stable but requires monitoring"
        elif critical_alerts > 2 or high_risk_fips > 3:
            overall_status = "degraded"
            status_description = "System experiencing significant issues"
        else:
            overall_status = "critical"
            status_description = "System requires immediate attention"
        
        return {
            'overall_system_status': overall_status,
            'status_description': status_description,
            'key_statistics': {
                'total_fips_analyzed': len(predictions),
                'high_risk_fips': high_risk_fips,
                'medium_risk_fips': medium_risk_fips,
                'critical_alerts': critical_alerts,
                'warning_alerts': warning_alerts,
                'average_system_health': round(avg_health, 1)
            },
            'business_impact': {
                'users_at_risk': total_users_at_risk,
                'revenue_at_risk_inr': total_revenue_risk,
                'estimated_recovery_time': self._estimate_recovery_time(predictions),
                'business_continuity_status': 'at_risk' if high_risk_fips > 2 else 'stable'
            },
            'immediate_actions_required': critical_alerts > 0 or high_risk_fips > 0,
            'next_critical_review': self._calculate_next_review_time(overall_status),
            'confidence_level': self._calculate_overall_confidence(predictions),
            'trend_direction': self._determine_trend_direction(comprehensive_report)
        }
    
    def _extract_immediate_concerns(self, patterns: Dict, predictions: Dict[str, PredictionResult]) -> List[Dict]:
        """Extract immediate concerns that need attention"""
        concerns = []
        
        # High probability predictions
        for fip_name, pred in predictions.items():
            if pred.downtime_probability > 0.7:
                concerns.append({
                    'type': 'high_risk_prediction',
                    'fip_name': fip_name,
                    'probability': pred.downtime_probability,
                    'time_window': pred.time_window,
                    'urgency': 'immediate',
                    'action_required': pred.recommended_actions[0] if pred.recommended_actions else 'Monitor closely'
                })
        
        # System-wide issues
        if 'system_wide_patterns' in patterns:
            system_patterns = patterns['system_wide_patterns']
            if system_patterns.get('overall_health_trend') == 'degrading':
                concerns.append({
                    'type': 'system_degradation',
                    'description': 'Overall system health trending downward',
                    'urgency': 'high',
                    'action_required': 'Investigate system-wide performance issues'
                })
        
        return sorted(concerns, key=lambda x: {'immediate': 0, 'high': 1, 'medium': 2}.get(x['urgency'], 3))
    
    def _generate_quick_recommendations(self, predictions: Dict[str, PredictionResult]) -> List[Dict]:
        """Generate quick actionable recommendations"""
        recommendations = []
        
        # High priority actions
        high_risk_fips = [fip for fip, pred in predictions.items() if pred.downtime_probability > 0.7]
        if high_risk_fips:
            recommendations.append({
                'priority': 'high',
                'action': f"Activate manual processing for {', '.join(high_risk_fips)}",
                'timeline': 'immediate',
                'impact': 'prevents user service disruption'
            })
        
        # Medium priority actions
        medium_risk_fips = [fip for fip, pred in predictions.items() if 0.4 <= pred.downtime_probability <= 0.7]
        if medium_risk_fips:
            recommendations.append({
                'priority': 'medium',
                'action': f"Increase monitoring frequency for {', '.join(medium_risk_fips)}",
                'timeline': 'next 2 hours',
                'impact': 'early detection of potential issues'
            })
        
        # Proactive measures
        recommendations.append({
            'priority': 'low',
            'action': 'Review and update emergency contact procedures',
            'timeline': 'next 24 hours',
            'impact': 'improved incident response time'
        })
        
        return recommendations
    
    def _calculate_risk_summary(self, predictions: Dict[str, PredictionResult]) -> Dict[str, Any]:
        """Calculate overall risk summary"""
        if not predictions:
            return {'overall_risk': 'unknown', 'risk_score': 0}
        
        # Calculate weighted risk score
        total_risk = sum(pred.downtime_probability for pred in predictions.values())
        avg_risk = total_risk / len(predictions)
        
        # Count risk levels
        high_risk = sum(1 for pred in predictions.values() if pred.downtime_probability > 0.7)
        medium_risk = sum(1 for pred in predictions.values() if 0.4 <= pred.downtime_probability <= 0.7)
        low_risk = len(predictions) - high_risk - medium_risk
        
        # Determine overall risk level
        if high_risk > len(predictions) * 0.3:
            overall_risk = 'critical'
        elif high_risk > 0 or medium_risk > len(predictions) * 0.5:
            overall_risk = 'high'
        elif medium_risk > 0:
            overall_risk = 'medium'
        else:
            overall_risk = 'low'
        
        return {
            'overall_risk': overall_risk,
            'risk_score': round(avg_risk * 100, 1),
            'distribution': {
                'high_risk_fips': high_risk,
                'medium_risk_fips': medium_risk,
                'low_risk_fips': low_risk
            },
            'top_risk_fips': sorted(
                [(fip, pred.downtime_probability) for fip, pred in predictions.items()],
                key=lambda x: x[1],
                reverse=True
            )[:3]
        }
    
    def _extract_key_metrics(self, analysis: AnalyticsResult) -> Dict[str, Any]:
        """Extract key metrics from analysis for reporting"""
        return {
            'analysis_period': analysis.time_range_analyzed,
            'system_availability': analysis.summary['key_statistics']['average_system_health'],
            'total_alerts_generated': len(analysis.proactive_alerts),
            'predictions_made': len(analysis.predictions),
            'maintenance_windows_detected': sum(len(windows) for windows in analysis.maintenance_windows.values()),
            'business_impact_summary': analysis.summary['business_impact'],
            'model_confidence': analysis.summary['confidence_level']
        }
    
    def _extract_critical_issues(self, analysis: AnalyticsResult) -> List[Dict]:
        """Extract critical issues from analysis"""
        critical_issues = []
        
        # Critical alerts
        for alert in analysis.proactive_alerts:
            if alert.severity == 'critical':
                critical_issues.append({
                    'type': 'critical_alert',
                    'fip_name': alert.fip_name,
                    'issue': alert.message,
                    'action_required': alert.recommended_action,
                    'confidence': alert.confidence
                })
        
        # High probability predictions
        for fip_name, pred in analysis.predictions.items():
            if pred.downtime_probability > 0.8:
                critical_issues.append({
                    'type': 'high_risk_prediction',
                    'fip_name': fip_name,
                    'issue': f"High downtime probability ({pred.downtime_probability:.1%})",
                    'action_required': pred.recommended_actions[0] if pred.recommended_actions else 'Immediate attention needed',
                    'confidence': pred.confidence_level
                })
        
        return critical_issues
    
    def _summarize_predictions(self, predictions: Dict[str, PredictionResult]) -> Dict[str, Any]:
        """Summarize predictions for reporting"""
        if not predictions:
            return {}
        
        avg_probability = sum(pred.downtime_probability for pred in predictions.values()) / len(predictions)
        
        risk_distribution = {
            'critical': sum(1 for pred in predictions.values() if pred.downtime_probability > 0.8),
            'high': sum(1 for pred in predictions.values() if 0.6 <= pred.downtime_probability <= 0.8),
            'medium': sum(1 for pred in predictions.values() if 0.3 <= pred.downtime_probability < 0.6),
            'low': sum(1 for pred in predictions.values() if pred.downtime_probability < 0.3)
        }
        
        return {
            'total_predictions': len(predictions),
            'average_risk_probability': round(avg_probability, 3),
            'risk_distribution': risk_distribution,
            'highest_risk_fip': max(predictions.items(), key=lambda x: x[1].downtime_probability)[0],
            'most_stable_fip': min(predictions.items(), key=lambda x: x[1].downtime_probability)[0],
            'next_predicted_event': self._find_next_predicted_event(predictions)
        }
    
    def _prioritize_actions(self, analysis: AnalyticsResult) -> List[Dict]:
        """Prioritize actions based on analysis results"""
        actions = []
        
        # Immediate actions from critical alerts
        for alert in analysis.proactive_alerts:
            if alert.severity == 'critical':
                actions.append({
                    'priority': 1,
                    'category': 'immediate',
                    'action': alert.recommended_action,
                    'fip_name': alert.fip_name,
                    'deadline': 'immediate',
                    'business_impact': 'high'
                })
        
        # Short-term actions from high-risk predictions
        for fip_name, pred in analysis.predictions.items():
            if pred.downtime_probability > 0.6:
                for action in pred.recommended_actions:
                    actions.append({
                        'priority': 2,
                        'category': 'short_term',
                        'action': action,
                        'fip_name': fip_name,
                        'deadline': pred.time_window,
                        'business_impact': 'medium'
                    })
        
        # Strategic actions from business insights
        if 'strategic_recommendations' in analysis.business_insights:
            for rec in analysis.business_insights['strategic_recommendations'].get('infrastructure_investments', []):
                actions.append({
                    'priority': 3,
                    'category': 'strategic',
                    'action': rec['recommendation'],
                    'fip_name': 'system_wide',
                    'deadline': rec['timeline'],
                    'business_impact': rec['priority']
                })
        
        return sorted(actions, key=lambda x: x['priority'])
    
    def _assess_data_quality(self, analysis: AnalyticsResult) -> Dict[str, Any]:
        """Assess the quality of data used in analysis"""
        return {
            'time_range_coverage': analysis.time_range_analyzed,
            'data_completeness': 'good',  # Would calculate from actual data
            'data_freshness': 'current',
            'missing_data_percentage': 5.2,  # Would calculate from actual data
            'data_quality_score': 0.87,
            'recommendations': [
                'Improve data collection frequency for real-time insights',
                'Add data validation checks for anomaly detection',
                'Implement data backup procedures for critical metrics'
            ]
        }
    
    def _calculate_model_confidence(self, analysis: AnalyticsResult) -> Dict[str, Any]:
        """Calculate overall model confidence"""
        if not analysis.predictions:
            return {'overall_confidence': 'unknown'}
        
        confidence_levels = [pred.confidence_level for pred in analysis.predictions.values()]
        confidence_scores = {
            'high': confidence_levels.count('high'),
            'medium': confidence_levels.count('medium'),
            'low': confidence_levels.count('low')
        }
        
        # Calculate weighted confidence score
        weights = {'high': 1.0, 'medium': 0.7, 'low': 0.4}
        total_weight = sum(weights[level] * count for level, count in confidence_scores.items())
        overall_confidence = total_weight / len(confidence_levels) if confidence_levels else 0
        
        return {
            'overall_confidence': 'high' if overall_confidence > 0.8 else 'medium' if overall_confidence > 0.6 else 'low',
            'confidence_score': round(overall_confidence, 2),
            'confidence_distribution': confidence_scores,
            'factors_affecting_confidence': [
                'Data quality and completeness',
                'Historical pattern consistency',
                'Model training data volume',
                'External factor correlation'
            ]
        }
    
    def _categorize_alerts(self, alerts: List[Alert]) -> Dict[str, Any]:
        """Categorize alerts for technical analysis"""
        categories = {}
        for alert in alerts:
            category = alert.alert_type
            if category not in categories:
                categories[category] = []
            categories[category].append({
                'fip_name': alert.fip_name,
                'severity': alert.severity,
                'message': alert.message,
                'confidence': alert.confidence
            })
        
        return {
            'total_alerts': len(alerts),
            'by_category': categories,
            'by_severity': {
                'critical': len([a for a in alerts if a.severity == 'critical']),
                'warning': len([a for a in alerts if a.severity == 'warning']),
                'info': len([a for a in alerts if a.severity == 'info'])
            },
            'average_confidence': sum(a.confidence for a in alerts) / len(alerts) if alerts else 0
        }
    
    def _estimate_recovery_time(self, predictions: Dict[str, PredictionResult]) -> str:
        """Estimate system recovery time based on predictions"""
        high_risk_count = sum(1 for pred in predictions.values() if pred.downtime_probability > 0.7)
        
        if high_risk_count == 0:
            return "No recovery needed - system stable"
        elif high_risk_count <= 2:
            return "2-4 hours with proactive measures"
        elif high_risk_count <= 5:
            return "4-8 hours with coordinated response"
        else:
            return "8-24 hours requiring emergency procedures"
    
    def _calculate_next_review_time(self, status: str) -> str:
        """Calculate when next review should occur"""
        if status == 'critical':
            return (datetime.utcnow() + timedelta(hours=1)).isoformat()
        elif status == 'degraded':
            return (datetime.utcnow() + timedelta(hours=4)).isoformat()
        elif status == 'stable_with_concerns':
            return (datetime.utcnow() + timedelta(hours=12)).isoformat()
        else:
            return (datetime.utcnow() + timedelta(hours=24)).isoformat()
    
    def _calculate_overall_confidence(self, predictions: Dict[str, PredictionResult]) -> str:
        """Calculate overall confidence level"""
        if not predictions:
            return 'unknown'
        
        high_confidence = sum(1 for pred in predictions.values() if pred.confidence_level == 'high')
        confidence_ratio = high_confidence / len(predictions)
        
        if confidence_ratio > 0.8:
            return 'high'
        elif confidence_ratio > 0.6:
            return 'medium'
        else:
            return 'low'
    
    def _determine_trend_direction(self, comprehensive_report: Dict) -> str:
        """Determine overall system trend direction"""
        system_patterns = comprehensive_report.get('historical_patterns', {}).get('system_wide_patterns', {})
        trend = system_patterns.get('overall_health_trend', 'stable')
        
        return trend
    
    def _find_next_predicted_event(self, predictions: Dict[str, PredictionResult]) -> Dict[str, Any]:
        """Find the next most likely predicted event"""
        if not predictions:
            return {}
        
        # Find prediction with highest probability and shortest time window
        next_event = max(predictions.items(), key=lambda x: x[1].downtime_probability)
        
        return {
            'fip_name': next_event[0],
            'probability': next_event[1].downtime_probability,
            'time_window': next_event[1].time_window,
            'confidence': next_event[1].confidence_level
        }
    
    # ================================
    # ADVANCED ANALYTICS METHODS
    # ================================
    
    async def generate_correlation_analysis(self, 
                                          days_back: int = 30,
                                          correlation_threshold: float = 0.7) -> Dict[str, Any]:
        """
        Analyze correlations between FIP performance metrics
        
        Args:
            days_back: Number of days to analyze for correlations
            correlation_threshold: Minimum correlation coefficient to report
            
        Returns:
            Correlation analysis between FIPs and metrics
        """
        
        self.logger.info(f"ðŸ”— Analyzing FIP correlations for {days_back} days")
        
        try:
            # Extract historical data
            historical_data = self.historical_analyzer.extract_historical_data(
                days_back=days_back,
                step="1h"
            )
            
            import pandas as pd
            import numpy as np
            
            correlations = {}
            
            # Analyze correlations for each metric type
            for metric_name, df in historical_data.items():
                if df.empty:
                    continue
                    
                # Pivot data to have FIPs as columns
                pivot_df = df.pivot_table(
                    index='timestamp', 
                    columns='fip_name', 
                    values='value',
                    aggfunc='mean'
                ).fillna(method='ffill').fillna(method='bfill')
                
                if len(pivot_df.columns) < 2:
                    continue
                
                # Calculate correlation matrix
                corr_matrix = pivot_df.corr()
                
                # Extract significant correlations
                significant_correlations = []
                for i, fip1 in enumerate(corr_matrix.columns):
                    for j, fip2 in enumerate(corr_matrix.columns):
                        if i < j:  # Avoid duplicates
                            corr_value = corr_matrix.iloc[i, j]
                            if not np.isnan(corr_value) and abs(corr_value) >= correlation_threshold:
                                significant_correlations.append({
                                    'fip_pair': [fip1, fip2],
                                    'correlation': round(corr_value, 3),
                                    'relationship': 'positive' if corr_value > 0 else 'negative',
                                    'strength': 'strong' if abs(corr_value) > 0.8 else 'moderate'
                                })
                
                correlations[metric_name] = {
                    'correlation_matrix': corr_matrix.round(3).to_dict(),
                    'significant_correlations': significant_correlations,
                    'total_correlations_found': len(significant_correlations)
                }
            
            # Generate insights
            insights = self._generate_correlation_insights(correlations)
            
            return {
                'timestamp': datetime.utcnow().isoformat(),
                'analysis_period': f"{days_back} days",
                'correlation_threshold': correlation_threshold,
                'correlations_by_metric': correlations,
                'insights': insights,
                'recommendations': self._generate_correlation_recommendations(correlations)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Error in correlation analysis: {e}")
            raise
    
    async def generate_capacity_planning(self, 
                                       forecast_days: int = 30,
                                       growth_scenarios: List[str] = None) -> Dict[str, Any]:
        """
        Generate capacity planning recommendations based on historical trends
        
        Args:
            forecast_days: Number of days to forecast ahead
            growth_scenarios: List of growth scenarios to model
            
        Returns:
            Capacity planning analysis and recommendations
        """
        
        self.logger.info(f"ðŸ“Š Generating capacity planning for {forecast_days} days")
        
        if growth_scenarios is None:
            growth_scenarios = ['conservative', 'moderate', 'aggressive']
        
        try:
            # Extract longer historical data for trend analysis
            historical_data = self.historical_analyzer.extract_historical_data(
                days_back=90,  # 3 months for trend analysis
                step="1d"  # Daily aggregation for capacity planning
            )
            
            capacity_analysis = {}
            
            for metric_name, df in historical_data.items():
                if df.empty or metric_name not in ['consent_success_rate', 'total_requests']:
                    continue
                
                # Analyze trends by FIP
                fip_capacity = {}
                for fip_name in df['fip_name'].unique():
                    fip_df = df[df['fip_name'] == fip_name].copy()
                    if len(fip_df) < 10:  # Need sufficient data
                        continue
                    
                    # Calculate trends
                    fip_df['days_from_start'] = (fip_df.index - fip_df.index.min()).days
                    
                    # Simple linear regression for trend
                    import numpy as np
                    if len(fip_df) > 1:
                        slope, intercept = np.polyfit(fip_df['days_from_start'], fip_df['value'], 1)
                        
                        # Generate forecasts for different scenarios
                        forecasts = {}
                        base_growth = slope
                        
                        for scenario in growth_scenarios:
                            if scenario == 'conservative':
                                growth_multiplier = 0.5
                            elif scenario == 'moderate':
                                growth_multiplier = 1.0
                            elif scenario == 'aggressive':
                                growth_multiplier = 2.0
                            else:
                                growth_multiplier = 1.0
                            
                            # Project forward
                            future_days = fip_df['days_from_start'].max() + forecast_days
                            projected_value = intercept + (base_growth * growth_multiplier * future_days)
                            
                            forecasts[scenario] = {
                                'projected_value': max(0, projected_value),
                                'growth_rate': base_growth * growth_multiplier,
                                'confidence': 'high' if abs(slope) < 0.1 else 'medium' if abs(slope) < 0.5 else 'low'
                            }
                        
                        fip_capacity[fip_name] = {
                            'current_trend': 'increasing' if slope > 0 else 'decreasing' if slope < 0 else 'stable',
                            'trend_strength': abs(slope),
                            'forecasts': forecasts,
                            'capacity_recommendations': self._generate_capacity_recommendations(fip_name, forecasts, metric_name)
                        }
                
                capacity_analysis[metric_name] = fip_capacity
            
            # Generate system-wide capacity insights
            system_capacity = self._generate_system_capacity_insights(capacity_analysis)
            
            return {
                'timestamp': datetime.utcnow().isoformat(),
                'forecast_period': f"{forecast_days} days",
                'growth_scenarios': growth_scenarios,
                'fip_capacity_analysis': capacity_analysis,
                'system_capacity_insights': system_capacity,
                'investment_recommendations': self._generate_investment_recommendations(capacity_analysis),
                'risk_assessment': self._assess_capacity_risks(capacity_analysis)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Error in capacity planning: {e}")
            raise
    
    async def generate_anomaly_report(self, 
                                    days_back: int = 14,
                                    anomaly_sensitivity: str = "medium") -> Dict[str, Any]:
        """
        Generate comprehensive anomaly detection report
        
        Args:
            days_back: Number of days to analyze for anomalies
            anomaly_sensitivity: Sensitivity level (low, medium, high)
            
        Returns:
            Detailed anomaly analysis report
        """
        
        self.logger.info(f"ðŸ” Generating anomaly report for {days_back} days")
        
        try:
            # Extract historical data
            historical_data = self.historical_analyzer.extract_historical_data(
                days_back=days_back,
                step="15m"  # High resolution for anomaly detection
            )
            
            # Calculate features for anomaly detection
            fip_features = self.historical_analyzer.calculate_features(historical_data)
            
            # Set sensitivity thresholds
            sensitivity_thresholds = {
                'low': {'z_score': 3.0, 'iqr_multiplier': 2.0},
                'medium': {'z_score': 2.5, 'iqr_multiplier': 1.5},
                'high': {'z_score': 2.0, 'iqr_multiplier': 1.0}
            }
            
            thresholds = sensitivity_thresholds.get(anomaly_sensitivity, sensitivity_thresholds['medium'])
            
            anomaly_report = {}
            
            for fip_name, features in fip_features.items():
                anomaly_features = features.get('anomaly_features', {})
                
                # Analyze anomalies for this FIP
                fip_anomalies = {
                    'total_anomalies': 0,
                    'anomaly_types': {},
                    'severity_distribution': {'high': 0, 'medium': 0, 'low': 0},
                    'temporal_patterns': {},
                    'impact_assessment': {}
                }
                
                for metric_name, anomaly_data in anomaly_features.items():
                    if isinstance(anomaly_data, dict):
                        total_anomalies = anomaly_data.get('total_anomalies', 0)
                        anomaly_rate = anomaly_data.get('anomaly_rate', 0)
                        recent_anomalies = anomaly_data.get('recent_anomalies', 0)
                        max_z_score = anomaly_data.get('max_z_score', 0)
                        
                        fip_anomalies['total_anomalies'] += total_anomalies
                        
                        # Classify anomaly severity
                        if max_z_score > 4.0 or anomaly_rate > 0.2:
                            severity = 'high'
                            fip_anomalies['severity_distribution']['high'] += 1
                        elif max_z_score > 3.0 or anomaly_rate > 0.1:
                            severity = 'medium'
                            fip_anomalies['severity_distribution']['medium'] += 1
                        else:
                            severity = 'low'
                            fip_anomalies['severity_distribution']['low'] += 1
                        
                        fip_anomalies['anomaly_types'][metric_name] = {
                            'total_anomalies': total_anomalies,
                            'anomaly_rate': round(anomaly_rate, 3),
                            'recent_anomalies': recent_anomalies,
                            'max_z_score': round(max_z_score, 2),
                            'severity': severity,
                            'trend': 'increasing' if recent_anomalies > total_anomalies * 0.3 else 'stable'
                        }
                
                # Generate insights for this FIP
                fip_anomalies['insights'] = self._generate_anomaly_insights(fip_name, fip_anomalies)
                fip_anomalies['recommendations'] = self._generate_anomaly_recommendations(fip_name, fip_anomalies)
                
                anomaly_report[fip_name] = fip_anomalies
            
            # Generate system-wide anomaly analysis
            system_anomalies = self._analyze_system_wide_anomalies(anomaly_report)
            
            return {
                'timestamp': datetime.utcnow().isoformat(),
                'analysis_period': f"{days_back} days",
                'sensitivity_level': anomaly_sensitivity,
                'fip_anomaly_analysis': anomaly_report,
                'system_wide_analysis': system_anomalies,
                'priority_actions': self._prioritize_anomaly_actions(anomaly_report),
                'monitoring_recommendations': self._generate_monitoring_recommendations(anomaly_report)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Error generating anomaly report: {e}")
            raise
    
    # ================================
    # HELPER METHODS FOR ADVANCED ANALYTICS
    # ================================
    
    def _generate_correlation_insights(self, correlations: Dict) -> List[str]:
        """Generate insights from correlation analysis"""
        insights = []
        
        for metric_name, correlation_data in correlations.items():
            significant_corrs = correlation_data.get('significant_correlations', [])
            
            if significant_corrs:
                insights.append(f"Found {len(significant_corrs)} significant correlations in {metric_name}")
                
                # Identify strongest correlations
                strongest = max(significant_corrs, key=lambda x: abs(x['correlation']))
                insights.append(
                    f"Strongest correlation: {strongest['fip_pair'][0]} and {strongest['fip_pair'][1]} "
                    f"({strongest['correlation']:.2f}) - {strongest['relationship']} {strongest['strength']}"
                )
        
        return insights
    
    def _generate_correlation_recommendations(self, correlations: Dict) -> List[str]:
        """Generate recommendations based on correlation analysis"""
        recommendations = []
        
        # Check for cascading failure risks
        for metric_name, correlation_data in correlations.items():
            strong_correlations = [
                corr for corr in correlation_data.get('significant_correlations', [])
                if abs(corr['correlation']) > 0.8
            ]
            
            if strong_correlations and metric_name in ['consent_success_rate', 'error_rate']:
                recommendations.append(
                    f"Monitor {metric_name} correlations for cascading failure prevention"
                )
        
        recommendations.extend([
            "Implement cross-FIP monitoring for correlated performance metrics",
            "Consider load balancing adjustments based on correlation patterns",
            "Set up early warning systems for correlated FIP degradation"
        ])
        
        return recommendations
    
    def _generate_capacity_recommendations(self, fip_name: str, forecasts: Dict, metric_name: str) -> List[str]:
        """Generate capacity recommendations for a specific FIP"""
        recommendations = []
        
        # Check if any scenario shows concerning trends
        aggressive_forecast = forecasts.get('aggressive', {})
        conservative_forecast = forecasts.get('conservative', {})
        
        if metric_name == 'total_requests':
            aggressive_value = aggressive_forecast.get('projected_value', 0)
            if aggressive_value > 1000:  # High request volume threshold
                recommendations.append(f"Consider scaling infrastructure for {fip_name} to handle projected load")
        
        elif metric_name == 'consent_success_rate':
            conservative_value = conservative_forecast.get('projected_value', 100)
            if conservative_value < 70:  # Declining success rate
                recommendations.append(f"Investigate performance degradation trends for {fip_name}")
        
        return recommendations
    
    def _generate_system_capacity_insights(self, capacity_analysis: Dict) -> Dict[str, Any]:
        """Generate system-wide capacity insights"""
        
        # Count FIPs with concerning trends
        declining_fips = []
        growing_load_fips = []
        
        for metric_name, fip_data in capacity_analysis.items():
            for fip_name, fip_capacity in fip_data.items():
                trend = fip_capacity.get('current_trend', 'stable')
                
                if metric_name == 'consent_success_rate' and trend == 'decreasing':
                    declining_fips.append(fip_name)
                elif metric_name == 'total_requests' and trend == 'increasing':
                    growing_load_fips.append(fip_name)
        
        return {
            'system_health_trend': 'concerning' if len(declining_fips) > 3 else 'stable',
            'fips_with_declining_performance': declining_fips,
            'fips_with_growing_load': growing_load_fips,
            'capacity_utilization': 'high' if len(growing_load_fips) > 5 else 'normal',
            'scaling_requirements': {
                'immediate': len([fip for fip in growing_load_fips if fip in declining_fips]),
                'short_term': len(growing_load_fips),
                'long_term': 'strategic_planning_needed' if len(declining_fips) > 2 else 'current_capacity_sufficient'
            }
        }
    
    def _generate_investment_recommendations(self, capacity_analysis: Dict) -> List[Dict]:
        """Generate investment recommendations based on capacity analysis"""
        recommendations = []
        
        # Infrastructure investments
        recommendations.append({
            'category': 'infrastructure',
            'recommendation': 'Implement auto-scaling capabilities for high-load FIPs',
            'priority': 'high',
            'estimated_cost': 'â‚¹5-8 lakhs',
            'timeline': '2-3 months',
            'expected_benefit': 'Automatic capacity adjustment based on demand'
        })
        
        # Monitoring investments
        recommendations.append({
            'category': 'monitoring',
            'recommendation': 'Deploy predictive capacity monitoring system',
            'priority': 'medium',
            'estimated_cost': 'â‚¹3-5 lakhs',
            'timeline': '1-2 months',
            'expected_benefit': 'Early warning for capacity constraints'
        })
        
        return recommendations
    
    def _assess_capacity_risks(self, capacity_analysis: Dict) -> Dict[str, Any]:
        """Assess capacity-related risks"""
        
        high_risk_fips = []
        medium_risk_fips = []
        
        for metric_name, fip_data in capacity_analysis.items():
            for fip_name, fip_capacity in fip_data.items():
                forecasts = fip_capacity.get('forecasts', {})
                aggressive = forecasts.get('aggressive', {})
                
                if metric_name == 'consent_success_rate':
                    projected_value = aggressive.get('projected_value', 100)
                    if projected_value < 50:
                        high_risk_fips.append(fip_name)
                    elif projected_value < 70:
                        medium_risk_fips.append(fip_name)
        
        return {
            'overall_risk_level': 'high' if len(high_risk_fips) > 2 else 'medium' if len(medium_risk_fips) > 3 else 'low',
            'high_risk_fips': high_risk_fips,
            'medium_risk_fips': medium_risk_fips,
            'risk_factors': [
                'Declining FIP performance trends',
                'Increasing load without scaling',
                'Lack of redundancy for critical FIPs'
            ],
            'mitigation_strategies': [
                'Implement proactive scaling policies',
                'Establish FIP performance SLAs',
                'Deploy backup FIP connections'
            ]
        }
    
    def _generate_anomaly_insights(self, fip_name: str, anomalies: Dict) -> List[str]:
        """Generate insights for FIP anomaly analysis"""
        insights = []
        
        total_anomalies = anomalies.get('total_anomalies', 0)
        severity_dist = anomalies.get('severity_distribution', {})
        
        if total_anomalies > 20:
            insights.append(f"{fip_name} shows high anomaly frequency ({total_anomalies} detected)")
        
        if severity_dist.get('high', 0) > 2:
            insights.append(f"Multiple high-severity anomalies detected in {fip_name}")
        
        # Check for trending anomalies
        anomaly_types = anomalies.get('anomaly_types', {})
        trending_metrics = [
            metric for metric, data in anomaly_types.items()
            if data.get('trend') == 'increasing'
        ]
        
        if trending_metrics:
            insights.append(f"Increasing anomaly trend in {', '.join(trending_metrics)}")
        
        return insights
    
    def _generate_anomaly_recommendations(self, fip_name: str, anomalies: Dict) -> List[str]:
        """Generate recommendations for anomaly handling"""
        recommendations = []
        
        high_severity = anomalies.get('severity_distribution', {}).get('high', 0)
        
        if high_severity > 0:
            recommendations.append(f"Immediate investigation required for {fip_name} high-severity anomalies")
        
        recommendations.extend([
            f"Implement enhanced monitoring for {fip_name}",
            f"Review {fip_name} system logs for root cause analysis",
            f"Consider adjusting {fip_name} performance thresholds"
        ])
        
        return recommendations
    
    def _analyze_system_wide_anomalies(self, anomaly_report: Dict) -> Dict[str, Any]:
        """Analyze system-wide anomaly patterns"""
        
        total_system_anomalies = sum(
            fip_data.get('total_anomalies', 0) 
            for fip_data in anomaly_report.values()
        )
        
        high_severity_fips = [
            fip_name for fip_name, fip_data in anomaly_report.items()
            if fip_data.get('severity_distribution', {}).get('high', 0) > 0
        ]
        
        return {
            'total_system_anomalies': total_system_anomalies,
            'high_severity_fips': high_severity_fips,
            'system_anomaly_rate': total_system_anomalies / len(anomaly_report) if anomaly_report else 0,
            'anomaly_distribution': {
                'fips_with_anomalies': len([f for f in anomaly_report.values() if f.get('total_anomalies', 0) > 0]),
                'total_fips_analyzed': len(anomaly_report)
            },
            'trending_issues': self._identify_trending_anomalies(anomaly_report)
        }
    
    def _identify_trending_anomalies(self, anomaly_report: Dict) -> List[str]:
        """Identify trending anomaly patterns across FIPs"""
        trending_issues = []
        
        # Check for common patterns
        consent_issues = []
        response_time_issues = []
        
        for fip_name, fip_data in anomaly_report.items():
            anomaly_types = fip_data.get('anomaly_types', {})
            
            if 'consent_success_rate' in anomaly_types:
                if anomaly_types['consent_success_rate'].get('trend') == 'increasing':
                    consent_issues.append(fip_name)
            
            if 'response_time' in anomaly_types:
                if anomaly_types['response_time'].get('trend') == 'increasing':
                    response_time_issues.append(fip_name)
        
        if len(consent_issues) > 2:
            trending_issues.append(f"System-wide consent approval anomalies affecting {len(consent_issues)} FIPs")
        
        if len(response_time_issues) > 2:
            trending_issues.append(f"System-wide response time anomalies affecting {len(response_time_issues)} FIPs")
        
        return trending_issues
    
    def _prioritize_anomaly_actions(self, anomaly_report: Dict) -> List[Dict]:
        """Prioritize actions based on anomaly analysis"""
        actions = []
        
        for fip_name, fip_data in anomaly_report.items():
            high_severity = fip_data.get('severity_distribution', {}).get('high', 0)
            total_anomalies = fip_data.get('total_anomalies', 0)
            
            if high_severity > 2:
                actions.append({
                    'priority': 1,
                    'fip_name': fip_name,
                    'action': f"Immediate investigation of {fip_name} high-severity anomalies",
                    'urgency': 'critical',
                    'expected_resolution_time': '2-4 hours'
                })
            elif total_anomalies > 15:
                actions.append({
                    'priority': 2,
                    'fip_name': fip_name,
                    'action': f"Review {fip_name} performance patterns and adjust monitoring",
                    'urgency': 'high',
                    'expected_resolution_time': '1-2 days'
                })
        
        return sorted(actions, key=lambda x: x['priority'])
    
    def _generate_monitoring_recommendations(self, anomaly_report: Dict) -> List[str]:
        """Generate monitoring recommendations based on anomaly patterns"""
        recommendations = [
            "Implement real-time anomaly detection with configurable sensitivity",
            "Set up automated alerts for high-severity anomaly patterns",
            "Deploy machine learning-based anomaly detection for pattern recognition",
            "Establish anomaly investigation playbooks for operations team",
            "Create anomaly trend dashboards for proactive monitoring"
        ]
        
        # Add specific recommendations based on findings
        high_anomaly_fips = [
            fip_name for fip_name, fip_data in anomaly_report.items()
            if fip_data.get('total_anomalies', 0) > 20
        ]
        
        if high_anomaly_fips:
            recommendations.append(
                f"Implement enhanced monitoring for high-anomaly FIPs: {', '.join(high_anomaly_fips)}"
            )
        
        return recommendations
    
########################################################
########################################################

    def get_historical_data(self,days_back:int = 30, step:str = "15m") -> Dict:
        """
        Get historical data for a list of FIPs
        """
        self.logger.info(f"ðŸ”Getting historical data for {days_back} days")
        try:
            historical_data = self.historical_analyzer.extract_historical_data(days_back, step)

            if not any(not df.empty for df in historical_data.values()):
                raise Exception("No historical data available for analysis")
            
            return historical_data
        except Exception as e:
            self.logger.error(f"âŒ Error getting historical data: {e}")
            raise

    def get_fip_features(self,historical_data:Dict) -> Dict:
        """
        Get FIP features from historical data
        """
        self.logger.info("ðŸ”Getting FIP features from historical data")
        try:
            fip_features = self.historical_analyzer.calculate_features(historical_data)
            return fip_features
        except Exception as e:
            self.logger.error(f"âŒ Error getting FIP features: {e}")
            raise

    async def predict_downtime_events(self,fips:List[str], prediction_horizon: str = "24h") -> Dict:
        """
        Predict downtime events for a list of FIPs
        """
        self.logger.info(f"ðŸ”Predicting downtime events for {fips} for {prediction_horizon}")
        
        try:
            # Step 1: Extract historical data
            self.logger.info("ðŸ“Š Extracting historical metrics...")
            historical_data = self.historical_analyzer.extract_historical_data(
                days_back=30,
                step="15m"  # 15-minute resolution for detailed analysis
            )
            
            if not any(not df.empty for df in historical_data.values()):
                raise Exception("No historical data available for analysis")
            
            # Step 2: Calculate features and patterns
            self.logger.info("ðŸ§® Calculating ML features...")
            fip_features = self.historical_analyzer.calculate_features(historical_data)
            
            # Step 3: Detect maintenance windows
            self.logger.info("ðŸ”§ Detecting maintenance patterns...")
            maintenance_windows = self.historical_analyzer.detect_maintenance_windows(historical_data)
            
            # Step 4: AI Pattern Analysis using Bedrock
            self.logger.info("ðŸ”® Generating AI predictions...")
            predictions = self.enhanced_bedrock_service.predict_downtime_events(
                historical_data, prediction_horizon
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Error in comprehensive analysis: {e}")
            raise

    async def generate_proactive_alerts(self,fips:List[str], prediction_horizon: str = "24h") -> Dict:
        """
        Generate proactive alerts for a list of FIPs
        """
        pass